











# Importing libraries
import pandas as pd
import os
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
import numpy as np

warnings.filterwarnings('ignore')





# Defining path to the dataset folder
data_path = 'data'

# Loading core datasets 
occupation_df = pd.read_excel(f'{data_path}/Occupation Data.xlsx')
interests_df = pd.read_excel(f'{data_path}/Interests.xlsx')
skills_df = pd.read_excel(f'{data_path}/Skills.xlsx')
education_df = pd.read_excel(f'{data_path}/Education, Training, and Experience.xlsx')
related_df = pd.read_excel(f'{data_path}/Related Occupations.xlsx')
abilities_df = pd.read_excel(f'{data_path}/Abilities.xlsx')
emerging_tasks_df = pd.read_excel(f'{data_path}/Emerging Tasks.xlsx')
edu_categories_df = pd.read_excel(f'{data_path}/Education, Training, and Experience Categories.xlsx')








# View structure of each DataFrame
print("Occupations:", occupation_df.shape)
print("Interests:", interests_df.shape)
print("Skills:", skills_df.shape)
print("Education:", education_df.shape)
print("Related Jobs:", related_df.shape)
print("Abilities:", abilities_df.shape)
print("Emerging Tasks:", emerging_tasks_df.shape)
print("Education Categories:", edu_categories_df.shape)

# Display sample rows
occupation_df.head(3)


occupation_df.head()
interests_df.head()
skills_df.head()
education_df.head()
related_df.head()
abilities_df.head()





# See column names
print(occupation_df.columns)
print('\n----------\n')  
print(interests_df.columns)
print('\n----------\n')
print(skills_df.columns)
print('\n----------\n')
print(education_df.columns)
print('\n----------\n')
print(related_df.columns)
print('\n----------\n')








# Standardize 'O*NET-SOC Code' column name
for df in [occupation_df, interests_df, skills_df, education_df, related_df, emerging_tasks_df, edu_categories_df]:
    df.rename(columns={'O*NET-SOC Code': 'ONET_Code'}, inplace=True) # Renaming for clarity

# Standardize abilities columns
abilities_df = abilities_df[['O*NET-SOC Code', 'Element Name', 'Scale ID', 'Data Value']]
abilities_df.rename(columns={'O*NET-SOC Code': 'ONET_Code', 'Element Name': 'Ability'}, inplace=True)
abilities_df = abilities_df[abilities_df['Scale ID'] == 'IM']  # Only Importance scale
abilities_df.drop(columns='Scale ID', inplace=True)














# Pivoting the interests to get one row per job with RIASEC scores
riasec_df = interests_df.pivot_table(index='ONET_Code',
                                     columns='Element Name',
                                     values='Data Value').reset_index()

# Renaming columns for consistency
riasec_df.columns.name = None  # To remove multiindex
riasec_df.rename(columns={
    'Realistic': 'R',
    'Investigative': 'I',
    'Artistic': 'A',
    'Social': 'S',
    'Enterprising': 'E',
    'Conventional': 'C'
}, inplace=True)

# Preview RIASEC vectors
riasec_df.head()








# Exploring what's in skills_df
skills_df['Scale ID'].value_counts()








# Filtering only 'Importance' scores
important_skills_df = skills_df[skills_df['Scale ID'] == 'IM']

# Keeping top 10 most important skills per job
top_skills_df = important_skills_df.groupby('ONET_Code', group_keys=False).apply(
    lambda x: x.sort_values(by='Data Value', ascending=False).head(10)
).reset_index(drop=True)

# Preview of top skills for one job
top_skills_df[top_skills_df['ONET_Code'] == '11-1011.00']








# Top 10 Skills for a Sample Job
onet_id = '11-1011.00'  
job_skills = top_skills_df[top_skills_df['ONET_Code'] == onet_id]

plt.figure(figsize=(14, 6))
sns.barplot(x='Data Value', y='Element Name', data=job_skills.sort_values('Data Value'))
plt.title("Top 10 Important Skills: Chief Executives")
plt.xlabel("Importance Score")
plt.ylabel("Skill")
plt.tight_layout()
plt.show()











# Merging education data with categories
education_df_with_cat = education_df.merge(
    edu_categories_df[['Element ID', 'Category', 'Category Description']],
    on='Element ID',
    how='left'
)


# Preview the merged DataFrame
print(education_df_with_cat.columns.tolist())





# Renaming columns for clarity with actual column names in the DataFrame
education_df_with_cat.rename(columns={
    'Category_y': 'Preparation Type',           # <-- correct source from merged df
    'Category Description': 'Preparation Level',
    'Element Name': 'Prep Component'
}, inplace=True)


# Preview DataFrame
education_df_with_cat[['ONET_Code', 'Prep Component', 'Preparation Type', 'Preparation Level']].head()





# Getting the most common (highest scoring) education level per job, now with category
edu_df_with_cat = (
    education_df_with_cat
    .sort_values(by='Data Value', ascending=False)
    .drop_duplicates(subset='ONET_Code')
    [['ONET_Code', 'Prep Component', 'Data Value', 'Preparation Type', 'Preparation Level']]
    .rename(columns={
        'Prep Component': 'Education Level',
        'Preparation Type': 'Education Category'
    })
)

# Preview the result
edu_df_with_cat.head()


education_df_with_cat.head()








# Creating full job profile
job_profiles = occupation_df.merge(riasec_df, on='ONET_Code', how='left')
job_profiles = job_profiles.merge(edu_df_with_cat, on='ONET_Code', how='left')

# Preview 
job_profiles[['Title', 'Description', 'Education Level', 'Education Category', 'Preparation Level']].head()








# Checking for duplicates
job_profiles.duplicated().sum()





# Checking missing values across all columns
job_profiles.isnull().sum()





# Dropping rows with missing education info
job_profiles_clean = job_profiles.dropna(subset=['Education Level', 'Education Category', 'Preparation Level'])





# Defining the interest columns
interest_cols = ['First Interest High-Point', 'Second Interest High-Point', 'Third Interest High-Point']

# Filling missing RIASEC interests with 'Unknown'
job_profiles_clean.loc[:, interest_cols] = job_profiles_clean[interest_cols].fillna('Unknown')


# Preview the cleaned DataFrame
job_profiles_clean


# Preview the cleaned DataFrame
job_profiles_clean.info()





from sklearn.preprocessing import MinMaxScaler

riasec_cols = ['R', 'I', 'A', 'S', 'E', 'C']
scaler = MinMaxScaler()
job_profiles_clean[riasec_cols] = scaler.fit_transform(job_profiles_clean[riasec_cols])
job_profiles_clean[riasec_cols]





# Normalize Education Score for filtering or weighted scoring.
scaler = MinMaxScaler()
job_profiles_clean.loc[:,'Normalized Education Score'] = scaler.fit_transform(
    job_profiles_clean[['Education Category']].astype(float)
)





# Checking unique values in Education Category and Preparation Level
print("Unique Education Category values:")
print(job_profiles_clean['Education Category'].sort_values().unique())

print("\nUnique Preparation Level values:")
print(job_profiles_clean['Preparation Level'].sort_values().unique())





# Creating an Education Level Dictionary
education_level_map = {
    1.0: "Less than High School",
    2.0: "High School Diploma or equivalent",
    3.0: "Post-Secondary Certificate",
    4.0: "Some College Courses",
    5.0: "Associate's Degree",
    6.0: "Bachelor's Degree",
    7.0: "Post-Baccalaureate Certificate",
    8.0: "Master's Degree",
    9.0: "Post-Master's Certificate",
    10.0: "First Professional Degree",
    11.0: "Doctoral Degree",
    12.0: "Post-Doctoral Training"
}

# Mapping the education levels to their corresponding codes
job_profiles_clean.loc[:,'Education Category Label'] = job_profiles_clean['Education Category'].map(education_level_map)
job_profiles_clean[['Education Category', 'Education Category Label', 'Preparation Level']].head(10)





# Check for any Education values with missing category labels
job_profiles_clean[job_profiles_clean['Education Category Label'].isnull()]





# Setting plot
plt.figure(figsize=(12, 8))
sns.set(font_scale=1.2)

# Creating the plot
sns.countplot(
    y='Education Category Label',
    data=job_profiles_clean,
    order=job_profiles_clean['Education Category Label'].value_counts().index,
    palette='Blues_r'  # Optional: adds color gradient
)

# Customizing titles and labels
plt.title("Distribution of Education Requirements (All Jobs)", fontsize=16)
plt.xlabel("Number of Jobs", fontsize=14)
plt.ylabel("Level of Education", fontsize=14)

# Improving layout
plt.tight_layout()

# Saving high-res image
plt.savefig('images/education_requirements_distribution.png', dpi=300)

# Show plot
plt.show()








# Defining RIASEC columns
riasec_cols = ['R', 'I', 'A', 'S', 'E', 'C']

# Setting plot style
sns.set(style="whitegrid")
plt.figure(figsize=(14, 6))

# Melting the DataFrame for seaborn
riasec_melted = job_profiles_clean.melt(
    value_vars=riasec_cols, 
    var_name='RIASEC Type', 
    value_name='Score'
)

# Plot distribution
sns.boxplot(
    x='RIASEC Type', 
    y='Score', 
    hue='RIASEC Type',
    data=riasec_melted, 
    palette='Set2', 
    legend=False
)

plt.title('Distribution of RIASEC Scores Across All Jobs', fontsize=14)
plt.ylabel('Score (0â€“7)')
plt.xlabel('RIASEC Type')
plt.grid(True)
plt.tight_layout()
plt.show()














from sklearn.metrics.pairwise import cosine_similarity





# RIASEC scores: [Realistic, Investigative, Artistic, Social, Enterprising, Conventional]
# Sample user RIASEC profile (scale: 0â€“7)
user_profile = {
    'R': 2.0,
    'I': 4.0,
    'A': 2.0,
    'S': 5.0,
    'E': 4.0,
    'C': 4.0
}





# Extracting job RIASEC features
job_riasec_vectors = job_profiles_clean[riasec_cols].values

# Converting user profile to vector(Normalizing by dividing each values by 7)
user_vector = np.array(list((score / 7 for score in user_profile.values()))).reshape(1, -1)





# Computing similarity between user and all job profiles
similarities = cosine_similarity(user_vector, job_riasec_vectors)

# Adding to DataFrame
job_profiles_clean.loc[:,'Similarity Score'] = similarities.flatten()
job_profiles_clean





# Setting style
sns.set_style("whitegrid")
sns.set_context("notebook", font_scale=1.2)

# Setting figure size
plt.figure(figsize=(12, 7))

# Plot the histogram with KDE
sns.histplot(
    data=job_profiles_clean,
    x='Similarity Score',
    bins=30,
    kde=True,
    color='#E63946',
    edgecolor='black'
)

# Tittle and labels
plt.title("Similarity Scores Between User and Job Profiles", fontsize=16, weight='bold')
plt.xlabel("Cosine Similarity Score", fontsize=14)
plt.ylabel("Number of Job Matches", fontsize=14)
plt.xticks(np.arange(0.4, 1.05, 0.05))  #

# Adding a grid for readability
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Improving layout 
plt.tight_layout()
plt.savefig('images/similarity_scores_distribution.png', dpi=300)

# Show plot
plt.show()








# Top 10 jobs that match user's RIASEC interests
top_matches = job_profiles_clean.sort_values(by='Similarity Score', ascending=False)

# Show selected columns
top_matches[['Title', 'Education Level', 'Preparation Level', 'Description', 'Similarity Score']].head(10)








# Example filter: Only Bachelor's Degree and below
filtered_matches = top_matches[
    top_matches['Education Category'] <= 6.0  # <= Bachelor's Degree
]

filtered_matches[['Title', 'Education Level', 'Preparation Level', 'Description', 'Similarity Score']].head(10)





# Defining similarity threshold
similarity_threshold = 0.95

# Filtering jobs based on similarity threshold and education level
sim_matches = filtered_matches[filtered_matches['Similarity Score'] >= similarity_threshold]

# Sorting the matches by similarity score
sim_matches = sim_matches.sort_values(by='Similarity Score', ascending=False)

# Show selected columns
sim_matches[['Title','Education Level', 'Preparation Level', 'Description', 'Similarity Score']].head(10)





# Selecting top matching job
top_job = sim_matches.iloc[0]

# RIASEC order
riasec_labels = ['R', 'I', 'A', 'S', 'E', 'C']

# Getting user and job scores
user_scores = user_vector.flatten().tolist()


job_scores = [top_job[col] for col in riasec_labels]

# Closing the loop
user_scores += [user_scores[0]]
job_scores += [job_scores[0]]
labels = riasec_labels + [riasec_labels[0]]

# Only generate angles for the 6 base points
angles = np.linspace(0, 2 * np.pi, len(riasec_labels), endpoint=False).tolist()
angles += [angles[0]]

# Plotting
plt.figure(figsize=(14, 8))
ax = plt.subplot(111, polar=True)

ax.plot(angles, user_scores, linewidth=2, linestyle='solid', label='User Profile')
ax.fill(angles, user_scores, alpha=0.25)

ax.plot(angles, job_scores, linewidth=2, linestyle='dashed', label=f"Top Job: {top_job['Title']}")
ax.fill(angles, job_scores, alpha=0.15)

ax.set_xticks(angles[:-1])
ax.set_xticklabels(riasec_labels)
ax.set_title('RIASEC Comparison: User vs Top Job Match', size=14, weight='bold', pad=50)
ax.legend(loc='upper right', bbox_to_anchor=(1.1, 1.1))

plt.tight_layout(pad=3)
plt.savefig('images/riasec_comparison_user_vs_job.png', dpi=300)
plt.show()











# Grouping skills per job
skills_per_job = education_df_with_cat.groupby('ONET_Code')['Prep Component'] \
                                      .apply(list).reset_index()
skills_per_job.columns = ['ONET_Code', 'Skill List']


# Merging into job_profiles_clean
job_profiles_clean = job_profiles_clean.merge(skills_per_job, on='ONET_Code', how='left')





from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')

# Exploding to long form and keeping ONET_Code
exploded_skills = job_profiles_clean[['ONET_Code', 'Skill List']].explode('Skill List')

# Encoding skills
skill_features = pd.DataFrame(
	encoder.fit_transform(exploded_skills[['Skill List']]),
	columns=encoder.get_feature_names_out(['Skill List'])
)
# Add ONET_Code back to skill_features for merging
skill_features['ONET_Code'] = exploded_skills['ONET_Code'].values

# Aggregate skill features by ONET_Code (sum, since one-hot)
skill_features = skill_features.groupby('ONET_Code', as_index=False).max()

# Merging into main DataFrame
job_profiles_clean = job_profiles_clean.drop(columns=['Skill List'], errors='ignore')
job_profiles_clean = job_profiles_clean.merge(skill_features, on='ONET_Code', how='left')


# Preview the final DataFrame structure
print("Final job profiles DataFrame structure:")
job_profiles_clean.head()





# Extracting RIASEC, Education, and Skills values for further processing

riasec_values = job_profiles_clean[riasec_cols].values
education_values = job_profiles_clean['Normalized Education Score'].values

# Extracting skill columns from job_profiles_clean
skill_cols = [col for col in job_profiles_clean.columns if col.startswith("Skill List_")]
skills_values = job_profiles_clean[skill_cols].values


# Displaying the shapes of the extracted values
print(f'The education value shape {education_values.shape}')
print(f'The skill values shape{skills_values.shape}')
print(f'The riasec values shape {riasec_values.shape}')





# Instantiate scaler
scaler = MinMaxScaler()

skill_values_scaled = scaler.fit_transform(skills_values)

riasec_w = 0.5
education_w = 0.2
skills_w = 0.3

# Ensure all are numpy arrays before multiplying
weighted_riasec = np.asarray(riasec_values) * riasec_w
weighted_education = np.asarray(education_values) * education_w
weighted_skills = np.asarray(skill_values_scaled) * skills_w

# Combining the weighted vectors
hybrid_matrix = np.hstack((weighted_riasec, weighted_education.reshape(-1, 1), weighted_skills))


hybrid_matrix.shape





# User profile
user_riasec = user_vector
user_edu_score = np.array(6/12).reshape(1, -1)  # Normalized
user_skills = np.zeros(skills_values.shape[1]).reshape(1, -1)  # No known learning skills


# Hybrid User Vector
user_hybrid_vector = np.hstack([
    user_riasec * riasec_w,
    user_skills * skills_w,
    user_edu_score * education_w
])


# Computing cosine Similarity between user and all jobs profiles
hybrid_similarity = cosine_similarity(user_hybrid_vector, hybrid_matrix)

# Storing in DataFrame
job_profiles_clean['Hybrid Similarity'] = hybrid_similarity.flatten()
top_hybrid_matches = job_profiles_clean.sort_values('Hybrid Similarity', ascending=False).head(10)





top_hybrid_matches = job_profiles_clean.sort_values('Hybrid Similarity', ascending=False).head(10)

# Show top matches with useful columns
# top_hybrid_matches[['Title', 'Education Category Label', 'Hybrid Similarity', 'Similarity Score', 'Normalized Education Score']].style.background_gradient(cmap='YlGn')
top_hybrid_matches[['Title', 'Description', 'Education Level', 'Preparation Level', 'Education Category Label', 'Hybrid Similarity', 'Similarity Score', 'Normalized Education Score']].style.background_gradient(cmap='YlGn')








from sklearn.metrics.pairwise import cosine_similarity

# Getting one-hot skill column names
skill_cols = encoder.get_feature_names_out(['Skill List']).tolist()

# Removing overlapping skill columns
existing_skill_cols = [col for col in skill_cols if col in top_hybrid_matches.columns]
top_hybrid_matches = top_hybrid_matches.drop(columns=existing_skill_cols, errors='ignore')

# Merging one-hot skill features
top_hybrid_matches = top_hybrid_matches.merge(skill_features, on='ONET_Code', how='left')

# Defining user's skill vector (e.g, all skills)
user_skill_vector = np.ones((1, len(skill_cols)))

# Computing cosine similarity
job_skill_matrix = top_hybrid_matches[skill_cols].values
skill_similarities = cosine_similarity(user_skill_vector, job_skill_matrix)[0]

# Adding to DataFrame
top_hybrid_matches['Skill Similarity'] = skill_similarities

# Getting top 10 matches and prep plot data
top_n = top_hybrid_matches.head(10).copy()
plot_data = top_n[['Similarity Score', 'Normalized Education Score', 'Skill Similarity']]
plot_data.index = top_n['Title']
plot_data['Total'] = plot_data.sum(axis=1)
plot_data = plot_data.sort_values('Total', ascending=True).drop(columns='Total')

# Plot
fig, ax = plt.subplots(figsize=(16, 10))
plot_data.plot(
    kind='barh',
    stacked=True,
    ax=ax,
    colormap='Set2',
    edgecolor='black',
    width=0.65
)

# Title and labels
ax.set_title("Top Job Recommendations â€“ Hybrid Score Breakdown", fontsize=18, fontweight='bold')
ax.set_xlabel("Cumulative Match Score", fontsize=13)
ax.set_ylabel("Job Title", fontsize=13)
ax.grid(axis='x', linestyle='--', linewidth=0.6, alpha=0.6)

# Font and spacing
plt.xticks(fontsize=11)
plt.yticks(fontsize=11)
plt.tight_layout(pad=3)

# Legend placement 
ax.legend(
    title="Score Component",
    loc='lower right',
    bbox_to_anchor=(0.01, 1.02),  # Fine-tune legend position
    fontsize=11,
    title_fontsize=12
)

# Show and save
plt.savefig('images/hybrid_score_decomposition.png', dpi=300)
plt.show()








#  Getting top 10 scores from both methods
top_a = top_matches[['Title', 'Similarity Score']].copy().head(10)
top_b = top_hybrid_matches[['Title', 'Similarity Score', 'Normalized Education Score', 'Skill Similarity']].copy().head(10)

# Computing Hybrid Score
top_b['Hybrid Similarity'] = top_b[['Similarity Score', 'Normalized Education Score', 'Skill Similarity']].sum(axis=1)

# Adding a ranking index for consistent plotting
top_a.reset_index(drop=True, inplace=True)
top_b.reset_index(drop=True, inplace=True)
top_a['Rank'] = top_a.index + 1
top_b['Rank'] = top_b.index + 1

# Merging by Rank
comparison_df = pd.merge(
    top_a[['Rank', 'Similarity Score']],
    top_b[['Rank', 'Hybrid Similarity']],
    on='Rank'
)
comparison_df.rename(columns={'Similarity Score': 'RIASEC Similarity'}, inplace=True)

# Plot
custom_colors = ['#1f77b4', '#ff7f0e']  # 
ax = comparison_df.plot(
    x='Rank',
    kind='bar',
    figsize=(14, 6),
    color=custom_colors,
    edgecolor='black'
)

# Titles and Labels
plt.title("Comparison: Cosine Similarity vs Hybrid Similarity Scores", fontsize=16, fontweight='bold')
plt.xlabel("Job Rank", fontsize=12)
plt.ylabel("Similarity Score", fontsize=12)
plt.xticks(rotation=0, fontsize=10)
plt.yticks(fontsize=10)
plt.grid(axis='y', linestyle='--', alpha=0.5)

# Legend in top-left
plt.legend(title="Score Type", loc='upper left', bbox_to_anchor=(0, 1.03), fontsize=10, title_fontsize=11)

# Save
plt.tight_layout()
plt.savefig('images/riasec_vs_hybrid_similarity_comparison.png', dpi=300)
plt.show()





# Getting top 10 scores from both methods
top_a = top_matches[['Title', 'Similarity Score']].copy().head(10)
top_b = top_hybrid_matches[['Title', 'Similarity Score', 'Normalized Education Score', 'Skill Similarity']].copy().head(10)

# Computing Hybrid Score
top_b['Hybrid Similarity'] = top_b[['Similarity Score', 'Normalized Education Score', 'Skill Similarity']].sum(axis=1)

# Resetting index and assigning rank
top_a = top_a.reset_index(drop=True)
top_b = top_b.reset_index(drop=True)
top_a['Rank'] = top_a.index + 1
top_b['Rank'] = top_b.index + 1

# Merging by Rank
comparison_df = pd.merge(
    top_a[['Rank', 'Similarity Score']],
    top_b[['Rank', 'Hybrid Similarity']],
    on='Rank'
)
comparison_df.rename(columns={'Similarity Score': 'RIASEC Similarity'}, inplace=True)

# Plotting line graph
plt.figure(figsize=(12, 6))
plt.plot(comparison_df['Rank'], comparison_df['RIASEC Similarity'], marker='o', linewidth=2, markersize=8, label='RIASEC Only', color='#1f77b4')
plt.plot(comparison_df['Rank'], comparison_df['Hybrid Similarity'], marker='s', linewidth=2, markersize=8, label='Hybrid Score', color='#ff7f0e')

# Title and labels
plt.title("Line Comparison: RIASEC vs Hybrid Similarity Scores", fontsize=16, fontweight='bold')
plt.xlabel("Job Rank", fontsize=12)
plt.ylabel("Similarity Score", fontsize=12)
plt.xticks(comparison_df['Rank'], labels=comparison_df['Rank'])
plt.grid(True, linestyle='--', alpha=0.5)
plt.legend(loc='upper left', title="Score Type", fontsize=10, title_fontsize=11)
plt.tight_layout()

# Show
plt.savefig("images/riasec_vs_hybrid_similarity_lineplot.png", dpi=300)
plt.show()








# Counting jobs before and after filtering
total_jobs = len(job_profiles)
filtered_jobs_count = len(job_profiles_clean['Normalized Education Score'] > (6/12))  # Assuming 6/12 is the threshold for Bachelor's Degree

# Bar plot
plt.bar(['Before Filter', 'After Filter'], [total_jobs, filtered_jobs_count], color=['#3498db', '#2ecc71'])
plt.title('Jobs Remaining After Education Filter')
plt.ylabel('Number of Jobs')
plt.tight_layout()
plt.savefig('images/jobs_remaining_after_education_filter.png', dpi=300)
plt.show()








abilities_df


# Defining User Skill Preferences
user_selected_skills = ['Deductive Reasoning', 'Information Ordering', 'Mathematical Reasoning']





from sklearn.metrics.pairwise import cosine_similarity
# Getting skill column names (already one-hot encoded)
skill_cols = [col for col in job_profiles_clean.columns if col.startswith('Skill List_')]

# Building user skill vector (assumes user selects all skills)
user_skill_vector = np.array([1] * len(skill_cols)).reshape(1, -1)

# Creating job skill matrix and fill any NaNs (if any)
job_skill_matrix = job_profiles_clean[skill_cols].fillna(0).values

# Computing cosine similarity
skill_similarities = cosine_similarity(user_skill_vector, job_skill_matrix)[0]

# Adding similarity scores back to the DataFrame
job_profiles_clean['Skill Similarity'] = skill_similarities


# Creating binary matrix where 1 = job requires this user-selected ability
job_ability_matrix = abilities_df[abilities_df['Ability'].isin(user_selected_skills)].copy()

# Pivot to get 1 row per job, columns per selected ability
job_ability_matrix['has_skill'] = 1
job_ability_matrix = job_ability_matrix.pivot_table(
    index='ONET_Code',
    columns='Ability',
    values='has_skill',
    fill_value=0
).reset_index()





# Building user ability vector (all 1s since user selected all these skills)
user_ability_vector = np.array([1] * len(user_selected_skills)).reshape(1, -1)

# Creating job ability matrix
job_ability_matrix_only = job_ability_matrix[user_selected_skills].values
ability_similarities = cosine_similarity(user_ability_vector, job_ability_matrix_only)[0]

# Adding similarity back to job_ability_matrix
job_ability_matrix['Job Ability Skill Similarity'] = ability_similarities





# Merging ability similarities back into main job_profiles_clean DataFrame
job_profiles_clean = job_profiles_clean.merge(
    job_ability_matrix[['ONET_Code', 'Job Ability Skill Similarity']],
    on='ONET_Code',
    how='left'
)

# Filling any unmatched jobs with 0 similarity
job_profiles_clean['Job Ability Skill Similarity'] = job_profiles_clean['Job Ability Skill Similarity'].fillna(0)


# Checking the required columns exist (they should based on your recent output)
assert 'Similarity Score' in job_profiles_clean.columns
assert 'Normalized Education Score' in job_profiles_clean.columns
assert 'Skill Similarity' in job_profiles_clean.columns
assert 'Job Ability Skill Similarity' in job_profiles_clean.columns





job_profiles_clean['Job Hybrid Similarity Score'] = (
    job_profiles_clean['Similarity Score'] +
    job_profiles_clean['Normalized Education Score'] +
    job_profiles_clean['Skill Similarity'] +
    job_profiles_clean['Job Ability Skill Similarity']
)





# Sorting by total hybrid score including job abilities
top_job_hybrid_matches = job_profiles_clean.sort_values(
    'Job Hybrid Similarity Score',
    ascending=False
).head(10)

# Display top job matches with key info
top_job_hybrid_matches[[
    'Title',
    'Description',
    'Education Level',
    'Preparation Level',
    'Education Category Label',
    'Job Hybrid Similarity Score',
    'Hybrid Similarity',
    'Similarity Score',
    'Normalized Education Score'
]].style.background_gradient(cmap='YlGn')











def get_user_profile():
    """
    Collect user profile input: RIASEC scores, education level, and selected skills.
    Returns:
        dict: User profile with RIASEC scores, education level (numeric), and list of skills.
    """

    # Step 1: RIASEC Input
    print("Enter your RIASEC Scores (scale of 0â€“7, separated by commas):")
    print("Format: R, I, A, S, E, C")
    r_i_a_s_e_c = input("Enter scores: ").strip().split(',')

    try:
        r, i, a, s, e, c = [float(score.strip()) for score in r_i_a_s_e_c]
    except ValueError:
        print("âš ï¸ Invalid RIASEC input. Defaulting to neutral scores.")
        r, i, a, s, e, c = [4.0] * 6

    # Step 2: Education Level Selection
    education_map = {
        1: "Less than High School",
        2: "High School Diploma or Equivalent",
        3: "Post-Secondary Certificate",
        4: "Some College Courses",
        5: "Associate Degree",
        6: "Bachelor's Degree",
        7: "Post-Baccalaureate's Degree",
        8: "Master's Degree",
        9: "Post-Master's Certificate",
        10: "First Professional Degree",
        11: "Doctoral Degree",
        12: "Post-Doctoral Training"
    }

    print("\nSelect your highest education level:")
    for k, v in education_map.items():
        print(f"{k}. {v}")
    
    edu_input = input("Enter the number corresponding to your education level: ").strip()
    try:
        education_level = int(edu_input)
        if education_level not in education_map:
            raise ValueError
    except ValueError:
        print("âš ï¸ Invalid input. Defaulting to Bachelor's Degree.")
        education_level = 6

    # Step 3: Skill Selection (up to 10)
    skill_map = {
        "1"  : 'Oral Comprehension',
        "2"  : 'Written Comprehension',
        "3"  : 'Oral Expression',
        "4"  : 'Written Expression',
        "5"  : 'Fluency of Ideas',
        "6"  : 'Originality',
        "7"  : 'Problem Sensitivity',
        "8"  : 'Deductive Reasoning',
        "9"  : 'Inductive Reasoning',
        "10" : 'Information Ordering',
        "11" : 'Category Flexibility',
        "12" : 'Mathematical Reasoning',
        "13" : 'Number Facility',
        "14" : 'Memorization',
        "15" : 'Speed of Closure',
        "16" : 'Flexibility of Closure',
        "17" : 'Perceptual Speed',
        "18" : 'Spatial Orientation',
        "19" : 'Visualization',
        "20" : 'Selective Attention',
        "21" : 'Time Sharing',
        "22" : 'Arm-Hand Steadiness',
        "23" : 'Manual Dexterity',
        "24" : 'Finger Dexterity',
        "25" : 'Control Precision',
        "26" : 'Multilimb Coordination',
        "27" : 'Response Orientation',
        "28" : 'Rate Control',
        "29" : 'Reaction Time',
        "30" : 'Wrist-Finger Speed',
        "31" : 'Speed of Limb Movement',
        "32" : 'Static Strength',
        "33" : 'Explosive Strength',
        "34" : 'Dynamic Strength',
        "35" : 'Trunk Strength',
        "36" : 'Stamina',
        "37" : 'Extent Flexibility',
        "38" : 'Dynamic Flexibility',
        "39" : 'Gross Body Coordination',
        "40" : 'Gross Body Equilibrium',
        "41" : 'Near Vision',
        "42" : 'Far Vision',
        "43" : 'Visual Color Discrimination',
        "44" : 'Night Vision',
        "45" : 'Peripheral Vision',
        "46" : 'Depth Perception',
        "47" : 'Glare Sensitivity',
        "48" : 'Hearing Sensitivity',
        "49" : 'Auditory Attention',
        "50" : 'Sound Localization',
        "51" : 'Speech Recognition',
        "52" : 'Speech Clarity'
    }

    print("\nSelect up to 10 skills you consider strong:")
    for k, v in skill_map.items():
        print(f"{k}. {v}")

    skill_input = input("Enter skill numbers separated by commas (e.g., 1,8,12): ").strip()
    user_skills = []
    if skill_input:
        user_skills = [skill_map[num.strip()] for num in skill_input.split(',') if num.strip() in skill_map]

    print("\nProfile Captured. Generating personalized recommendations...\n")

    return {
        'R': r, 'I': i, 'A': a, 'S': s, 'E': e, 'C': c,
        'education_level': education_level,
        'skills': user_skills
    }





#### user_profile = get_user_profile()

if user_profile:
    print("\n--- USER PROFILE SUMMARY ---")
    print(f"RIASEC Scores: R={user_profile['R']}, I={user_profile['I']}, A={user_profile['A']}, "
          f"S={user_profile['S']}, E={user_profile['E']}, C={user_profile['C']}")
    print("Education Level:", user_profile['education_level'])
    print("Selected Skills:", user_profile['skills'])








def generate_recommendations(user_profile, job_profiles_clean):
    """
    Generate top 10 job recommendations based on RIASEC, education, and skill similarity.
    """

    # RIASEC Similarity
    riasec_cols = ['R', 'I', 'A', 'S', 'E', 'C']
    user_vector = np.array([
        user_profile['R'], user_profile['I'], user_profile['A'],
        user_profile['S'], user_profile['E'], user_profile['C']
    ]).reshape(1, -1)

    job_vectors = job_profiles_clean[riasec_cols].values
    riasec_similarities = cosine_similarity(user_vector, job_vectors)[0]
    job_profiles_clean['User RIASEC Similarity'] = riasec_similarities

    # Education Match Score - already included as 'Normalized Education Score' (0 to 1 scale)

    # Skill Similarity
    skill_cols = [col for col in job_profiles_clean.columns if col.startswith("Skill List_")]
    
    # Converting user skill names to vector
    user_skills = user_profile.get('skills', [])
    user_skill_vector = np.zeros((1, len(skill_cols)))

    for i, skill in enumerate(skill_cols):
        skill_name = skill.replace("Skill List_", "").lower()
        if any(skill_name in s.lower() for s in user_skills):
            user_skill_vector[0][i] = 1

    job_skill_matrix = job_profiles_clean[skill_cols].fillna(0).values
    skill_similarities = cosine_similarity(user_skill_vector, job_skill_matrix)[0]
    job_profiles_clean['User Skill Similarity'] = skill_similarities

    # Final Hybrid Score 
    job_profiles_clean['Hybrid Recommendation Score'] = (
        job_profiles_clean['User RIASEC Similarity'] +
        job_profiles_clean['Normalized Education Score'] +
        job_profiles_clean['User Skill Similarity']
    )

    # Top 10 Recommendations 
    top_matches = job_profiles_clean.sort_values('Hybrid Recommendation Score', ascending=False).head(10)

    # Returning selected columns for display
    return top_matches[[
        'Title', 'Description', 'Education Level', 'Preparation Level',
        'Education Category Label', 'Hybrid Recommendation Score',
        'User RIASEC Similarity', 'Normalized Education Score', 'User Skill Similarity'
    ]]





# Collecting user input
user = get_user_profile()

Generating recommendations based on user input and your cleaned job dataset
recommendations = generate_recommendations(user, job_profiles_clean)
recommendations.style.background_gradient(cmap='YlGn')





top_matches = generate_recommendations(user_profile, job_profiles_clean)

get_ipython().run_line_magic("matplotlib", " inline")

def plot_recommendation_breakdown(top_matches):
    """
    Plots the breakdown of Hybrid Recommendation Score for top job matches.
    """
    import matplotlib.pyplot as plt

    # Select and format relevant data
    plot_data = top_matches[[
        'Title', 'User RIASEC Similarity', 'Normalized Education Score', 'User Skill Similarity'
    ]].copy()
    
    plot_data.set_index('Title', inplace=True)
    plot_data.sort_values(by='User RIASEC Similarity', ascending=False, inplace=True)

    # Plot settings
    ax = plot_data.plot(kind='bar', stacked=True, figsize=(14, 8), colormap='YlGnBu')
    plt.title('ðŸ“Š Hybrid Recommendation Score Breakdown for Top Jobs', fontsize=14)
    plt.ylabel('Total Score')
    plt.xlabel('Job Title')
    plt.xticks(rotation=45, ha='right')
    plt.legend(loc='lower right', bbox_to_anchor=(1.15, 1))
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.tight_layout()
    plt.show()


job_profiles_with_skills = job_profiles_clean.copy()

# Save
output_path_1 = 'data/job_profiles_with_skills.csv'
job_profiles_with_skills.to_csv(output_path_1, index=False)








#Load the crosswalk file, skip metadata rows, manually set columns
crosswalk_df = pd.read_excel('data\Perkins_IV_Crosswalk_Table_4_ONETs_in_Pathways.xls',skiprows=5, header=None)
crosswalk_df.columns = ['Code','Title', 'Space']
#drop space
crosswalk_df = crosswalk_df[['Code','Title']]
crosswalk_df.head()





#convert code column to str
crosswalk_df['Code']=crosswalk_df['Code'].astype(str)

#extract cluster names and fill them downwards
crosswalk_df['Cluster'] = crosswalk_df['Title'].where(crosswalk_df['Code'].str.fullmatch(r'^\d+$')).ffill()

#extract pathways names and fill them downwards
crosswalk_df['Pathway'] = crosswalk_df['Title'].where(crosswalk_df['Code'].str.fullmatch(r'^\d+\.\d+$')).ffill()

#filter rows with ONET codes only
career_cluster = crosswalk_df[crosswalk_df['Code'].str.match(r'^\d{2}-\d{4}\.\d{2}$')].copy()

#rename the columne to match jobprofile
career_cluster = career_cluster.rename(columns={'Code': 'ONET_Code',})[['ONET_Code','Title','Pathway', 'Cluster']]

career_cluster = career_cluster.reset_index(drop=True)
career_cluster.head()


career_cluster.info()


#check duplicates
career_cluster.duplicated(subset='ONET_Code').any()


#drop duplicates
career_cluster = career_cluster.drop_duplicates(subset='ONET_Code', keep='first')
career_cluster.duplicated(subset='ONET_Code').any()





#merging dataset to create a new one 
job_profiles_cleaned = job_profiles_clean.merge(career_cluster.drop(columns=['Title']), on='ONET_Code', how='left')
job_profiles_cleaned.head()


#checking mapped careers
mapped = job_profiles_cleaned['Cluster'].notna().sum()
total = len(job_profiles_cleaned)
print(f"{mapped} out of {total} occupations were mapped to clusters/pathways.")


# create a copy for mapped profiles only to be used for supervised modelling
mapped_job_profiles = job_profiles_cleaned[job_profiles_cleaned['Cluster'].notna()].copy()





#### Encoding the target column cluster then naming it cluster_encoded

from sklearn.preprocessing import LabelEncoder

# Make a copy to avoid modifying original
df_encoded = mapped_job_profiles.copy()

# Label Encode the Target Column 'Cluster'
le_cluster = LabelEncoder()
df_encoded['Cluster_Encoded'] = le_cluster.fit_transform(df_encoded['Cluster'])

# check encoding
for i, label in enumerate(le_cluster.classes_):
    print(f"{i}: {label}")

# drop the original categorical columns
df_encoded_model = df_encoded.drop(columns=[
    'Cluster' 
])



## This is the encoded data 
df_encoded_model.head()





# Drop non-numeric columns first (eg'Title', 'Description', 'ONET_Code', 'Pathway')
numeric_df = df_encoded_model.select_dtypes(include='number')

# Calculate correlation with the target column then sort 
target_corr = numeric_df.corr()['Cluster_Encoded'].drop('Cluster_Encoded').sort_values(ascending=False)

# Show top 20 most correlated features
print(target_corr.head(20))


#selecting features with â‰¥ 0.03

selected_features = [
    'Third Interest High-Point',
    'R',
    'C',
    'Second Interest High-Point',
    'I',
    'Normalized Education Score',
    'Education Category',
    'Hybrid Similarity',
    'Data Value',
    'Similarity Score',
    'User RIASEC Similarity',
    'Hybrid Recommendation Score',
    'Job Hybrid Similarity Score',
    'E',
    'First Interest High-Point',
    'S',
    'A',
    'Cluster_Encoded' # Target
]

model_ready = df_encoded_model[selected_features].copy()
model_ready.head()





#imports
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, accuracy_score

# assign features and target
X = model_ready.drop(columns=['Cluster_Encoded'])
y = model_ready['Cluster_Encoded']

# Remove classes with only 1 sample in y
(unique, counts) = np.unique(y, return_counts=True)
classes_to_keep = unique[counts > 1]
mask = np.isin(y, classes_to_keep)

X_filtered = X[mask].reset_index(drop=True)
y_filtered = y[mask].reset_index(drop=True)


 #check for correlation between the features and drop any that is >0.95
corr_matrix = X_filtered.corr().abs()
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))

# Find features with correlation > 0.95
high_corr_features = [col for col in upper.columns if any(upper[col] > 0.95)]
print("Highly correlated features to drop:", high_corr_features)

# Drop those from X
X_filtered = X_filtered.drop(columns=high_corr_features)



# splitting the data
X_train, X_test, y_train,y_test = train_test_split(X_filtered, y_filtered, test_size=0.2, random_state=42, stratify=y_filtered)

#pipeline with scaling
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('logreg', LogisticRegression(
        max_iter=1000,
        multi_class='multinomial',
        solver='lbfgs',
        class_weight='balanced'
    ))
])

 #Train model
pipeline.fit(X_train, y_train)


#predict
y_pred = pipeline.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:")
print(classification_report(y_test, y_pred))








from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline


# assign features and target
X = model_ready.drop(columns=['Cluster_Encoded'])
y = model_ready['Cluster_Encoded']

# Remove classes with only 1 sample in y
(unique, counts) = np.unique(y, return_counts=True)
classes_to_keep = unique[counts > 1]
mask = np.isin(y, classes_to_keep)

X_filtered = X[mask].reset_index(drop=True)
y_filtered = y[mask].reset_index(drop=True)


#check for correlation between the features and drop any that is >0.95
corr_matrix = X_filtered.corr().abs()
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))

# Find features with correlation > 0.95
high_corr_features = [col for col in upper.columns if any(upper[col] > 0.95)]
print("Highly correlated features to drop:", high_corr_features)

# Drop those from X
X_filtered = X_filtered.drop(columns=high_corr_features)


# splitting the data
X_train, X_test, y_train,y_test = train_test_split(X_filtered, y_filtered, test_size=0.2, random_state=42, stratify=y_filtered)

#pipeline with scaling
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('smote', SMOTE(random_state=42)),
    ('logreg', LogisticRegression(
        max_iter=1000,
        multi_class='multinomial',
        solver='lbfgs',
        class_weight='balanced'
    ))
])

 #Train model
pipeline.fit(X_train, y_train)


#predict and evaluate
y_pred = pipeline.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:")
print(classification_report(y_test, y_pred))








#import catboost
from catboost import CatBoostClassifier, cv,Pool

# assign features and target
X = model_ready.drop(columns=['Cluster_Encoded'])
y = model_ready['Cluster_Encoded']

# Remove classes with only 1 sample in y
(unique, counts) = np.unique(y, return_counts=True)
classes_to_keep = unique[counts > 1]
mask = np.isin(y, classes_to_keep)

X_filtered = X[mask].reset_index(drop=True)
y_filtered = y[mask].reset_index(drop=True)



categorical_features = [
    'First Interest High-Point',
    'Second Interest High-Point',
    'Third Interest High-Point',
    'Education Category'
]
#converting into strings because some values may not be whole numbers
for col in ['First Interest High-Point', 'Second Interest High-Point', 'Third Interest High-Point','Education Category']:
    X_filtered[col] = X_filtered[col].astype(str)


#split data
X_train, X_test, y_train,y_test = train_test_split(X_filtered, y_filtered, test_size=0.2, random_state=42, stratify=y_filtered)

#create pools to help handle categorical data better
train_pool = Pool(data=X_train, label=y_train, cat_features=categorical_features)
test_pool = Pool(data=X_test, label=y_test, cat_features=categorical_features)

#training the model
catboost_model = CatBoostClassifier(
    iterations=200,
    learning_rate=0.1,
    depth=6,
    loss_function='MultiClass',
    eval_metric='Accuracy',
    verbose=50,
    random_seed=42
)


#fit the model
catboost_model.fit(train_pool, eval_set=test_pool)


#predict the test set
y_pred = catboost_model.predict(X_test)
y_pred = y_pred.flatten()

print("Classification Report:")
print(classification_report(y_test, y_pred))





#checking class imbalance 

from collections import Counter

class_counts = Counter(y_filtered)
for cls, count in sorted(class_counts.items()):
    print(f"Class {cls}: {count} samples")








#perform cross validation
from sklearn.model_selection import StratifiedKFold

# create full Pool with categorical features
pooled = Pool(data=X_filtered, label=y_filtered, cat_features=categorical_features)

#define CV parameters
crossval_params = {
    'iterations': 500,
    'learning_rate': 0.1,
    'depth': 6,
    'loss_function': 'MultiClass',
    'eval_metric': 'Accuracy',
    'auto_class_weights': 'Balanced',
    'random_seed': 42
}
# Stratified CV
crossval_results = cv(
    params=crossval_params,
    pool=pooled,
    fold_count=5,
    stratified=True,
    verbose=50,
    early_stopping_rounds=50
)

# Check best iteration & accuracy
best_iteration = len(crossval_results['test-Accuracy-mean'])
best_accuracy = max(crossval_results['test-Accuracy-mean'])
print(f"Best iteration: {best_iteration}")
print(f"Best CV Accuracy: {best_accuracy:.4f}")





final_catboost = CatBoostClassifier(
    iterations=best_iteration,
    learning_rate=0.1,
    depth=6,
    loss_function='MultiClass',
    eval_metric='Accuracy',
    auto_class_weights='Balanced',
    random_seed=42,
    verbose=100
)

final_catboost.fit(
    X_filtered,
    y_filtered,
    cat_features=categorical_features
)



#predict
y_pred = final_catboost.predict(X_test)
y_pred = y_pred.flatten()

print("Classification Report:")
print(classification_report(y_test, y_pred))








from sklearn.ensemble import RandomForestClassifier

#split data
X_train, X_test, y_train,y_test = train_test_split(X_filtered, y_filtered, test_size=0.2, random_state=42, stratify=y_filtered)


# Initialize and train the Random Forest Classifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
rf_model.fit(X_train, y_train)

# Make predictions
y_pred = rf_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

print(f"Accuracy: {accuracy:.2f}")
print("Classification Report:\n", report)








from sklearn.model_selection import RandomizedSearchCV

#split data
X_train, X_test, y_train,y_test = train_test_split(X_filtered, y_filtered, test_size=0.2, random_state=42, stratify=y_filtered)

# defining parameter grid
param_dist = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5],
    'max_features': ['sqrt', 'log2']
}
#initializing random forest
rf = RandomForestClassifier(random_state=42)

#random search cv
random_cv = RandomizedSearchCV(
    estimator=rf,
    param_distributions=param_dist,
    n_iter=20,  
    cv=3,
    verbose=2,
    random_state=42,
    n_jobs=-1,
    scoring='accuracy'
)

#fit model
random_cv.fit(X_train, y_train)

# Best model
best_rf = random_cv.best_estimator_

# Predict and evaluate
y_pred = best_rf.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

print(f"Accuracy: {accuracy:.2f}")
print("Classification Report:\n", report)


best_rf = random_cv.best_estimator_
best_rf





from xgboost import XGBClassifier
# assign features and target
X = model_ready.drop(columns=['Cluster_Encoded'])
y = model_ready['Cluster_Encoded']

# Remove classes with only 1 sample in y
(unique, counts) = np.unique(y, return_counts=True)
classes_to_keep = unique[counts > 1]
mask = np.isin(y, classes_to_keep)

X_filtered = X[mask].reset_index(drop=True)
y_filtered = y[mask].reset_index(drop=True)

#split data
X_train, X_test, y_train,y_test = train_test_split(X_filtered, y_filtered, test_size=0.2, random_state=42, stratify=y_filtered)

#Filter out test classes not seen in training
valid_classes = np.unique(y_train)
test_mask = y_test.isin(valid_classes)
X_test = X_test[test_mask]
y_test = y_test[test_mask]

from sklearn.preprocessing import LabelEncoder

#remapping labels

le = LabelEncoder()
y_train_encoded = le.fit_transform(y_train)
y_test_encoded = le.transform(y_test)


# training the model
xgb_model = XGBClassifier(
    use_label_encoder=False,
    num_class=len(np.unique(y_train_encoded)),
    eval_metric='mlogloss', 
    random_state=42)

xgb_model.fit(X_train, y_train_encoded)


#predict and evaluate the model
y_pred = xgb_model.predict(X_test)

print(" Accuracy:", accuracy_score(y_test_encoded, y_pred))
print("Classification Report:")
print(classification_report(y_test_encoded, y_pred))








param_dist = {
    'n_estimators': [100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.05, 0.1],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}
#split data
X_train, X_test, y_train,y_test = train_test_split(X_filtered, y_filtered, test_size=0.2, random_state=42, stratify=y_filtered)

#Filter out test classes not seen in training
valid_classes = np.unique(y_train)
test_mask = y_test.isin(valid_classes)
X_test = X_test[test_mask]
y_test = y_test[test_mask]

from sklearn.preprocessing import LabelEncoder

#remapping labels

le = LabelEncoder()
y_train_encoded = le.fit_transform(y_train)
y_test_encoded = le.transform(y_test)


# training the model
xgb_model = XGBClassifier(
    use_label_encoder=False,
    num_class=len(np.unique(y_train_encoded)),
    eval_metric='mlogloss', 
    random_state=42)

xgb_model.fit(X_train, y_train_encoded)


# Perform RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=xgb_model,
    param_distributions=param_dist,
    n_iter=10, 
    scoring='accuracy',
    cv=3,
    verbose=1,
    random_state=42,
    n_jobs=-1
)

# Fit the model
random_search.fit(X_train, y_train_encoded)

# Best estimator
best_xgb_model = random_search.best_estimator_

# Predict
y_pred = best_xgb_model.predict(X_test)

# Decode predictions
y_pred_decoded = le.inverse_transform(y_pred)

#predict and evaluate the model
y_pred = best_xgb_model.predict(X_test)

print(" Accuracy:", accuracy_score(y_test_encoded, y_pred))
print("Classification Report:")
print(classification_report(y_test_encoded, y_pred))





from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder

# Define feature types for encoding and scaling
categorical_cols = ['First Interest High-Point', 'Second Interest High-Point', 'Third Interest High-Point', 'Education Category']
numeric_cols = [col for col in X_train.columns if col not in categorical_cols]

# Create the preprocessor to ensure models are properly scaled
preprocessor = ColumnTransformer([
    ('num', StandardScaler(), numeric_cols),
    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)
])

# Wrap each model in a pipeline
xgb_pipeline = Pipeline([
    ('preprocess', preprocessor),
    ('model', best_xgb_model)
])

rf_pipeline = Pipeline([
    ('preprocess', preprocessor),
    ('model', best_rf)
])

cat_pipeline = Pipeline([
    ('preprocess', preprocessor),
    ('model', final_catboost)
])


#using soft voting
from sklearn.ensemble import VotingClassifier


ensemble_model = VotingClassifier(
    estimators=[
        ('logreg', pipeline),  # already a pipeline
        ('rf', rf_pipeline),
        ('xgb', xgb_pipeline),
        ('cat', cat_pipeline)
    ],
    voting='soft',
    weights=[2, 3, 4, 5] 
)
# training on the training set
ensemble_model.fit(X_train, y_train)

# Predicting on the test set
ensemble_preds = ensemble_model.predict(X_test)

# Evaluate
print(" Ensemble Accuracy:", accuracy_score(y_test, ensemble_preds))
print(" Ensemble Classification Report:\n", classification_report(y_test, ensemble_preds))













# Importing clustering libraries
import hdbscan
from sklearn.cluster import KMeans, DBSCAN
from sklearn.cluster import AgglomerativeClustering
from sklearn.decomposition import PCA

# saving the job abilities and skills dataset that will be used for modelling
output_path_1 = 'data/job_profiles_with_skills.csv'
job_profiles_with_skills.to_csv(output_path_1, index=False)





# Displaying all column names with their index positions
for i, col in enumerate(job_profiles_with_skills.columns):
    print(f"{i}: {col}")





def generate_recommendations(user_profile, job_profiles_clean, top_n=10):
    """
    Generate top job recommendations based on RIASEC, education, and skill similarity.
    
    Parameters:
    - user_profile: dict with keys ['R', 'I', 'A', 'S', 'E', 'C', 'education_level', 'skills']
    - job_profiles_clean: DataFrame with job info and skill indicators
    - top_n: number of job recommendations to return
    
    Returns:
    - DataFrame of top recommended jobs
    """

    # RIASEC Similarity
    riasec_cols = ['R', 'I', 'A', 'S', 'E', 'C']
    user_vector = np.array([
        user_profile['R'], user_profile['I'], user_profile['A'],
        user_profile['S'], user_profile['E'], user_profile['C']
    ]).reshape(1, -1)

    job_vectors = job_profiles_clean[riasec_cols].values
    job_profiles_clean['User RIASEC Similarity'] = cosine_similarity(user_vector, job_vectors)[0]

    # Education Score (already normalized, assumed 0 to 1)
    # Column: 'Normalized Education Score'

    # Skill Similarity (based on user input and binary columns)
    skill_cols = [col for col in job_profiles_clean.columns if col.startswith("Skill List_")]
    user_skills = [s.lower() for s in user_profile.get('skills', [])]

    # Creating user skill vector (binary 1/0)
    user_skill_vector = np.zeros((1, len(skill_cols)))
    for i, skill_col in enumerate(skill_cols):
        skill_name = skill_col.replace("Skill List_", "").lower()
        if skill_name in user_skills:
            user_skill_vector[0][i] = 1

    job_skill_matrix = job_profiles_clean[skill_cols].fillna(0).values
    job_profiles_clean['User Skill Similarity'] = cosine_similarity(user_skill_vector, job_skill_matrix)[0]

    # Final Hybrid Score
    job_profiles_clean['Hybrid Recommendation Score'] = (
        job_profiles_clean['User RIASEC Similarity'] +
        job_profiles_clean['Normalized Education Score'] +
        job_profiles_clean['User Skill Similarity']
    )

    # Top Recommendations
    top_matches = job_profiles_clean.sort_values('Hybrid Recommendation Score', ascending=False).head(top_n)

    # Return summary
    return top_matches[[  
        'Title', 'Description', 'Education Level', 'Preparation Level',
        'Education Category Label', 'Hybrid Recommendation Score',
        'User RIASEC Similarity', 'Normalized Education Score', 'User Skill Similarity'
    ]].style.background_gradient(cmap='YlGn')





user_profile = {
    'R': 5, 'I': 4, 'A': 2, 'S': 3, 'E': 1, 'C': 2,
    'education_level': 10,  # On a 0â€“12 scale
    'skills': ['on-the-job training', 'related work experience', 'professional certification']
}

recommendations = generate_recommendations(user_profile, job_profiles_clean)
recommendations





from sklearn.cluster import AgglomerativeClustering

def agglomerative_recommender(user_profile: dict,
                               job_profiles: pd.DataFrame,
                               n_clusters: int = 10,
                               top_n: int = 5) -> pd.DataFrame:
    """
    Recommends jobs using Agglomerative Clustering and includes individual similarity components.
    """

    # Defining features
    feature_cols = ['R', 'I', 'A', 'S', 'E', 'C', 'Normalized Education Score'] + \
                   job_profiles.columns[27:-1].tolist()
    X = job_profiles[feature_cols].fillna(0).values

    # Clustering
    model = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')
    labels = model.fit_predict(X)
    job_profiles['Cluster'] = labels

    # Normalizing and building user vector
    user_vector = np.array([
        user_profile['R'] / 7, user_profile['I'] / 7, user_profile['A'] / 7,
        user_profile['S'] / 7, user_profile['E'] / 7, user_profile['C'] / 7,
        user_profile['education_level'] / 12
    ] + [0] * (X.shape[1] - 7)).reshape(1, -1)

    # Assigning to nearest cluster
    cluster_centroids = [X[labels == i].mean(axis=0) for i in range(n_clusters)]
    cluster_similarities = cosine_similarity(user_vector, cluster_centroids)[0]
    user_cluster = np.argmax(cluster_similarities)

    # Selecting jobs in user's cluster
    cluster_jobs = job_profiles[job_profiles['Cluster'] == user_cluster].copy()

    # Similarities (component-wise) RIASEC Similarity
    riasec_cols = ['R', 'I', 'A', 'S', 'E', 'C']
    user_riasec = np.array([[user_profile[col] / 7 for col in riasec_cols]])
    job_riasec = cluster_jobs[riasec_cols].fillna(0).values
    cluster_jobs['User RIASEC Similarity'] = cosine_similarity(user_riasec, job_riasec)[0]

    # Skill Similarity
    skill_cols = [col for col in job_profiles.columns if col.startswith("Skill List_")]
    user_skills = user_profile.get('skills', [])
    user_skill_vector = np.zeros((1, len(skill_cols)))
    for i, col in enumerate(skill_cols):
        if any(col.replace("Skill List_", "").lower() in s.lower() for s in user_skills):
            user_skill_vector[0][i] = 1
    job_skill_matrix = cluster_jobs[skill_cols].fillna(0).values
    cluster_jobs['User Skill Similarity'] = cosine_similarity(user_skill_vector, job_skill_matrix)[0]

    # Final Hybrid Scores
    cluster_jobs['Hybrid Recommendation Score'] = (
        cluster_jobs['User RIASEC Similarity'] +
        cluster_jobs['Normalized Education Score'] +
        cluster_jobs['User Skill Similarity']
    )

    # Cosine similarity over full feature vector 
    sim_scores = cosine_similarity(user_vector, cluster_jobs[feature_cols].values).flatten()
    cluster_jobs['Similarity Score'] = sim_scores

    # Sorting and returning top results
    top_jobs = cluster_jobs.sort_values(by='Hybrid Recommendation Score', ascending=False).head(top_n)

    return top_jobs[[
        'Title', 'Description', 'Education Level', 'Preparation Level',
        'Education Category Label', 'Hybrid Recommendation Score',
        'User RIASEC Similarity', 'User Skill Similarity',
        'Normalized Education Score', 'Similarity Score'
    ]].style.background_gradient(cmap='YlGn')





# User Input
user_profile = {
    'R': 5, 'I': 3, 'A': 4, 'S': 2, 'E': 1, 'C': 1,
    'education_level': 10,
    'skills': ['professional certification', 'on-the-job training']
}

# Getting Recommendations
agglo_recommendations = agglomerative_recommender(user_profile, job_profiles_with_skills)
agglo_recommendations








from sklearn.cluster import DBSCAN

def dbscan_recommender(user_profile: dict, 
                       job_profiles: pd.DataFrame,
                       eps: float = 0.5,
                       min_samples: int = 5,
                       top_n: int = 5) -> pd.DataFrame:
    """
    Recommends jobs using DBSCAN clustering and includes RIASEC, skills, and education match scores.
    """

    # Defining features to cluster on
    feature_cols = ['R', 'I', 'A', 'S', 'E', 'C', 'Normalized Education Score'] + \
                   job_profiles.columns[27:-1].tolist()
    X = job_profiles[feature_cols].fillna(0).values

    # Fitting DBSCAN model
    dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='euclidean')
    labels = dbscan.fit_predict(X)
    job_profiles['Cluster'] = labels

    # Preparing user vector
    user_vector = np.array([
        user_profile['R'] / 7, user_profile['I'] / 7, user_profile['A'] / 7,
        user_profile['S'] / 7, user_profile['E'] / 7, user_profile['C'] / 7,
        user_profile['education_level'] / 12
    ] + [0] * (X.shape[1] - 7)).reshape(1, -1)

    # Selecting jobs from non-noise clusters
    valid_jobs = job_profiles[job_profiles['Cluster'] != -1].copy()

    # RIASEC Similarity
    riasec_cols = ['R', 'I', 'A', 'S', 'E', 'C']
    user_riasec = np.array([[user_profile[col] / 7 for col in riasec_cols]])
    job_riasec = valid_jobs[riasec_cols].fillna(0).values
    valid_jobs['User RIASEC Similarity'] = cosine_similarity(user_riasec, job_riasec)[0]

    # Skill Similarity
    skill_cols = [col for col in job_profiles.columns if col.startswith("Skill List_")]
    user_skills = user_profile.get('skills', [])
    user_skill_vector = np.zeros((1, len(skill_cols)))
    for i, col in enumerate(skill_cols):
        if any(col.replace("Skill List_", "").lower() in s.lower() for s in user_skills):
            user_skill_vector[0][i] = 1
    job_skill_matrix = valid_jobs[skill_cols].fillna(0).values
    valid_jobs['User Skill Similarity'] = cosine_similarity(user_skill_vector, job_skill_matrix)[0]

    # Final Hybrid Score
    valid_jobs['Hybrid Recommendation Score'] = (
        valid_jobs['User RIASEC Similarity'] +
        valid_jobs['Normalized Education Score'] +
        valid_jobs['User Skill Similarity']
    )

    # Cosine similarity with full vector 
    sim_scores = cosine_similarity(user_vector, valid_jobs[feature_cols].values).flatten()
    valid_jobs['Similarity Score'] = sim_scores

    # Top N jobs
    top_jobs = valid_jobs.sort_values('Hybrid Recommendation Score', ascending=False).head(top_n)

    return top_jobs[[
        'Title', 'Description', 'Education Level', 'Preparation Level',
        'Education Category Label', 'Hybrid Recommendation Score',
        'User RIASEC Similarity', 'User Skill Similarity',
        'Normalized Education Score', 'Similarity Score'
    ]].style.background_gradient(cmap='YlGn')





# Sample user profile
user_profile = {
    'R': 5, 'I': 3, 'A': 4, 'S': 2, 'E': 1, 'C': 1,
    'education_level': 10,
    'skills': ['on-the-job training', 'professional certification']
}

# Get recommendations from DBSCAN
dbscan_recommendations = dbscan_recommender(user_profile, job_profiles_with_skills)
dbscan_recommendations








import hdbscan

def hdbscan_recommender(user_profile: dict, 
                        job_profiles: pd.DataFrame,
                        min_cluster_size: int = 10,
                        top_n: int = 5) -> pd.DataFrame:
    """
    Recommends jobs using HDBSCAN clustering and hybrid similarity scoring (RIASEC + skills + education).
    """

    # Selecting Features for Clustering
    feature_cols = ['R', 'I', 'A', 'S', 'E', 'C', 'Normalized Education Score'] + \
                   job_profiles.columns[27:-1].tolist()
    X = job_profiles[feature_cols].fillna(0).values

    # HDBSCAN Clustering
    hdb = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, prediction_data=True)
    labels = hdb.fit_predict(X)
    job_profiles['Cluster'] = labels

    # Preparing user vector
    user_vector = np.array([
        user_profile['R'] / 7, user_profile['I'] / 7, user_profile['A'] / 7,
        user_profile['S'] / 7, user_profile['E'] / 7, user_profile['C'] / 7,
        user_profile['education_level'] / 12
    ] + [0] * (X.shape[1] - 7)).reshape(1, -1)

    # Filtering out noise
    clustered_jobs = job_profiles[job_profiles['Cluster'] != -1].copy()

    # RIASEC Similarity
    riasec_cols = ['R', 'I', 'A', 'S', 'E', 'C']
    user_riasec = np.array([[user_profile[col] / 7 for col in riasec_cols]])
    job_riasec = clustered_jobs[riasec_cols].fillna(0).values
    clustered_jobs['User RIASEC Similarity'] = cosine_similarity(user_riasec, job_riasec)[0]

    # Skill Similarity
    skill_cols = [col for col in job_profiles.columns if col.startswith("Skill List_")]
    user_skills = user_profile.get('skills', [])
    user_skill_vector = np.zeros((1, len(skill_cols)))
    for i, col in enumerate(skill_cols):
        if any(col.replace("Skill List_", "").lower() in s.lower() for s in user_skills):
            user_skill_vector[0][i] = 1
    job_skill_matrix = clustered_jobs[skill_cols].fillna(0).values
    clustered_jobs['User Skill Similarity'] = cosine_similarity(user_skill_vector, job_skill_matrix)[0]

    # Final Hybrid Score
    clustered_jobs['Hybrid Recommendation Score'] = (
        clustered_jobs['User RIASEC Similarity'] +
        clustered_jobs['Normalized Education Score'] +
        clustered_jobs['User Skill Similarity']
    )

    # Cosine Similarity with full feature vector
    sim_scores = cosine_similarity(user_vector, clustered_jobs[feature_cols].values).flatten()
    clustered_jobs['Similarity Score'] = sim_scores

    # Top N Jobs
    top_jobs = clustered_jobs.sort_values('Hybrid Recommendation Score', ascending=False).head(top_n)

    return top_jobs[[
        'Title', 'Description', 'Education Level', 'Preparation Level',
        'Education Category Label', 'Hybrid Recommendation Score',
        'User RIASEC Similarity', 'User Skill Similarity',
        'Normalized Education Score', 'Similarity Score'
    ]].style.background_gradient(cmap='YlGn')





# Sample user input
user_profile = {
    'R': 5, 'I': 3, 'A': 4, 'S': 2, 'E': 1, 'C': 1,
    'education_level': 10,
    'skills': ['professional certification', 'on-the-job training']
}

# Generating recommendations
hdbscan_recommendations = hdbscan_recommender(user_profile, job_profiles_with_skills)
hdbscan_recommendations








from sklearn.decomposition import PCA

# Selecting Features for PCA
pca_features = ['R', 'I', 'A', 'S', 'E', 'C', 'Normalized Education Score'] + \
               job_profiles_with_skills.columns[27:-1].tolist()

X = job_profiles_with_skills[pca_features].fillna(0).values

# Applying PCA
pca = PCA(n_components=2)  # Reduce to 2 components for visualization
X_pca = pca.fit_transform(X)

# Saving PCA Components to DataFrame
job_profiles_with_skills['PCA1'] = X_pca[:, 0]
job_profiles_with_skills['PCA2'] = X_pca[:, 1]

# Adding Clustering Labels 
if 'Cluster' not in job_profiles_with_skills.columns:
    from sklearn.cluster import KMeans
    km_model = KMeans(n_clusters=8, random_state=42)
    job_profiles_with_skills['Cluster'] = km_model.fit_predict(X)





# Visualizing the PCA Output
plt.figure(figsize=(10, 6))
sns.scatterplot(
    x='PCA1', y='PCA2',
    hue='Cluster',
    palette='Set2',
    data=job_profiles_with_skills,
    legend='full',
    alpha=0.7
)
plt.title('PCA of Job Profiles Based on Skills + RIASEC + Education')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.savefig('images/job_profiles_pca.png', dpi=300)
plt.show()








print("Explained variance ratio:", pca.explained_variance_ratio_)
print("Total variance explained:", pca.explained_variance_ratio_.sum())








from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

def visualize_all_clusters(job_profile_with_skills):
    """
    Visualize and compare the clustering results from KMeans, Agglomerative, DBSCAN, and HDBSCAN.
    Prints clustering performance metrics and displays scatter plots of the clusters.
    """

    # Selecting RIASEC + Education + Skill-related columns for clustering
    features = ['R', 'I', 'A', 'S', 'E', 'C', 'Normalized Education Score'] + \
               job_profile_with_skills.columns[27:-1].tolist()
    
    # Converting to matrix and handle missing values
    X = job_profile_with_skills[features].fillna(0).values

    # Defining clustering models
    clusterers = {
        'KMeans': KMeans(n_clusters=10, random_state=42),
        'Agglomerative': AgglomerativeClustering(n_clusters=10),
        'DBSCAN': DBSCAN(eps=1.0, min_samples=5),
        'HDBSCAN': hdbscan.HDBSCAN(min_cluster_size=5)
    }

    # Prepare 2x2 subplot layout
    fig, axs = plt.subplots(2, 2, figsize=(14, 12))
    axs = axs.flatten()

    for i, (name, model) in enumerate(clusterers.items()):
        # Predicting clusters
        labels = model.fit_predict(X)

        # Visualizing raw clusters using first two features (note: PCA is better for visual clarity)
        axs[i].scatter(X[:, 0], X[:, 1], c=labels, cmap='tab10', s=10)
        axs[i].set_title(f"{name} Clusters (Raw Feature Space)")
        axs[i].set_xlabel("Feature 1")
        axs[i].set_ylabel("Feature 2")

        # Calculating and displaying clustering evaluation metrics
        unique_labels = set(labels)
        if len(unique_labels) > 1 and -1 not in unique_labels:
            sil = silhouette_score(X, labels)
            db = davies_bouldin_score(X, labels)
            ch = calinski_harabasz_score(X, labels)

            print(f"\nðŸ“Š {name} Metrics:")
            print(f"  â€¢ Silhouette Score: {sil:.4f}")
            print(f"  â€¢ Davies-Bouldin Index: {db:.4f}")
            print(f"  â€¢ Calinski-Harabasz Score: {ch:.2f}")
        else:
            print(f"\n {name} produced one cluster or contains noise (unclustered points).")

    plt.tight_layout()
    plt.show()


visualize_all_clusters(job_profiles_with_skills)














from sklearn.metrics import silhouette_score

# Defining features for clustering
features = ['R', 'I', 'A', 'S', 'E', 'C', 'Normalized Education Score'] + job_profiles_with_skills.columns[27:-1].tolist()
X = job_profiles_with_skills[features].fillna(0).values

# Storing scores
inertia_scores = []
silhouette_scores = []
k_range = range(2, 15)

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(X)
    inertia_scores.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(X, labels))

# Plotting Inertia (Elbow Curve)
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
plt.plot(k_range, inertia_scores, marker='o')
plt.title("Elbow Method (Inertia)")
plt.xlabel("Number of Clusters (k)")
plt.ylabel("Inertia")

# Plotting Silhouette Scores
plt.subplot(1, 2, 2)
plt.plot(k_range, silhouette_scores, marker='s', color='green')
plt.title("Silhouette Score by Cluster Count")
plt.xlabel("Number of Clusters (k)")
plt.ylabel("Silhouette Score")

plt.tight_layout()
plt.show()








from sklearn.decomposition import PCA

# Reducing to 2 or 3 components for visualization or faster clustering
pca = PCA(n_components=2)
X_reduced = pca.fit_transform(X)

print("Explained variance ratio:", pca.explained_variance_ratio_)
print("Total variance explained:", sum(pca.explained_variance_ratio_))








# Choosing optimal k based on plots (e.g., 8)
optimal_k = 8
kmeans_final = KMeans(n_clusters=optimal_k, random_state=42)
labels = kmeans_final.fit_predict(X_reduced)

# Adding cluster labels back to dataset
job_profiles_with_skills['KMeans Cluster (PCA)'] = labels

# Visualizing clusters
plt.figure(figsize=(14, 6))
plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=labels, cmap='Set1', s=10)
plt.title(f"K-Means Clustering (k={optimal_k}) on PCA-Reduced Data")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.show()





from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score

sil = silhouette_score(X_reduced, labels)
db = davies_bouldin_score(X_reduced, labels)
ch = calinski_harabasz_score(X_reduced, labels)

print(f"Final KMeans Metrics (k={optimal_k}):")
print(f"Silhouette Score: {sil:.4f}")
print(f"Davies-Bouldin Index: {db:.4f}")
print(f"Calinski-Harabasz Score: {ch:.2f}")











# Defining features for PCA (RIASEC + Education + Skills)
features = ['R', 'I', 'A', 'S', 'E', 'C', 'Normalized Education Score'] + \
           job_profiles_with_skills.columns[27:-1].tolist()

# Preparing feature matrix
X = job_profiles_with_skills[features].fillna(0).values

# Fitting PCA and reduce to 2 components
pca_model = PCA(n_components=2)
X_reduced = pca_model.fit_transform(X)





def final_kmeans_recommender(user_profile: dict, job_profiles: pd.DataFrame, n_clusters: int = 8, top_n: int = 5):
    """
    Uses optimized KMeans clustering on PCA-reduced features AND computes individual similarity scores.
    """

    # Feature preparation 
    skill_cols = [col for col in job_profiles.columns if col.startswith("Skill List_")]
    features = ['R', 'I', 'A', 'S', 'E', 'C', 'Normalized Education Score'] + skill_cols
    X = job_profiles[features].fillna(0).values

    #  PCA transformation 
    pca_model = PCA(n_components=2)
    X_reduced = pca_model.fit_transform(X)

    # Fitting KMeans on reduced features 
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    cluster_labels = kmeans.fit_predict(X_reduced)
    job_profiles = job_profiles.copy()
    job_profiles['Cluster'] = cluster_labels

    # Creating user vector (original and reduced) 
    education_score = user_profile['education_level'] / 12
    user_raw_vector = np.array([
        user_profile['R']/7, user_profile['I']/7, user_profile['A']/7,
        user_profile['S']/7, user_profile['E']/7, user_profile['C']/7,
        education_score
    ])

    # Skill vector
    user_skills = user_profile.get('skills', [])
    user_skill_vector = np.zeros(len(skill_cols))
    for i, skill in enumerate(skill_cols):
        skill_name = skill.replace("Skill List_", "").lower()
        if any(skill_name in s.lower() for s in user_skills):
            user_skill_vector[i] = 1

    # Full vector
    user_full_vector = np.concatenate([user_raw_vector, user_skill_vector]).reshape(1, -1)

    # PCA-reduced
    user_vector_reduced = pca_model.transform(user_full_vector)

    #  Predicting user cluster
    user_cluster = kmeans.predict(user_vector_reduced)[0]
    cluster_jobs = job_profiles[job_profiles['Cluster'] == user_cluster].copy()

    # Computeing cosine similarity in PCA space 
    cluster_X_reduced = X_reduced[cluster_jobs.index]
    cosine_sim = cosine_similarity(user_vector_reduced, cluster_X_reduced).flatten()
    cluster_jobs['Cosine Similarity'] = cosine_sim

    # Computeing individual similarity scores

    # RIASEC
    riasec_cols = ['R', 'I', 'A', 'S', 'E', 'C']
    job_riasec = cluster_jobs[riasec_cols].values
    user_riasec = np.array([
        user_profile['R'], user_profile['I'], user_profile['A'],
        user_profile['S'], user_profile['E'], user_profile['C']
    ]).reshape(1, -1)
    riasec_sim = cosine_similarity(user_riasec, job_riasec)[0]
    cluster_jobs['User RIASEC Similarity'] = riasec_sim

    # Skill similarity
    job_skill_matrix = cluster_jobs[skill_cols].fillna(0).values
    skill_sim = cosine_similarity(user_skill_vector.reshape(1, -1), job_skill_matrix)[0]
    cluster_jobs['User Skill Similarity'] = skill_sim

    # Adding back normalized education (already in dataset)

    #  Final Hybrid Score 
    cluster_jobs['Hybrid Score'] = (
        cluster_jobs['User RIASEC Similarity'] +
        cluster_jobs['User Skill Similarity'] +
        cluster_jobs['Normalized Education Score']
    )

    # Returning top N recommendations
    top_matches = cluster_jobs.sort_values(by='Hybrid Score', ascending=False).head(top_n)

    return top_matches[[
        'Title', 'Description', 'Education Level', 'Preparation Level',
        'Education Category Label',
        'Hybrid Score', 'User RIASEC Similarity', 'Normalized Education Score',
        'User Skill Similarity', 'Cosine Similarity'
    ]]





# User Input
user_profile = {
    'R': 5, 'I': 4, 'A': 2, 'S': 3, 'E': 1, 'C': 2,
    'education_level': 10,
    'skills': ['job-related professional certification']
}

final_recommendations = final_kmeans_recommender(
    user_profile, job_profiles_with_skills, n_clusters=8
)

final_recommendations.style.background_gradient(cmap='YlGn')





import os
import pickle

# Creating the directory if it doesn't exist
os.makedirs("models", exist_ok=True)

# Saving the model
with open("models/kmeans_model.pkl", "wb") as f:
    pickle.dump(kmeans, f)

# Loading the model later
with open("models/kmeans_model.pkl", "rb") as f:
    kmeans = pickle.load(f)


import os

# Create the 'data' directory if it doesn't exist
os.makedirs("data", exist_ok=True)

# Save the DataFrame
job_profiles_clean.to_csv("data/job_profiles_clean.csv", index=False)

print("job_profiles_clean.csv saved successfully!")





















